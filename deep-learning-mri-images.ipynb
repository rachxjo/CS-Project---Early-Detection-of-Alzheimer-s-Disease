{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":5962731,"sourceType":"datasetVersion","datasetId":3419493}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"CNN","metadata":{}},{"cell_type":"code","source":"import os\nimport glob\nimport numpy as np\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split\n\n# Base directory where dataset is located\ndataset_dir = \"/kaggle/input/imagesoasis/Data\"\n\n# Automatically detect class folders (instead of hardcoding)\nclasses = sorted([d for d in os.listdir(dataset_dir) if os.path.isdir(os.path.join(dataset_dir, d))])\nprint(f\"Detected classes: {classes}\")\n\n# Load image paths and labels\nimage_paths, labels = [], []\nfor class_label, class_name in enumerate(classes):\n    class_dir = os.path.join(dataset_dir, class_name)\n    files = glob.glob(os.path.join(class_dir, \"*.jpg\"))\n    print(f\"Class: {class_name}, Files Found: {len(files)}\")\n    for file_path in files:\n        image_paths.append(file_path)\n        labels.append(class_label)\n\n# Check total images loaded\nprint(f\"\\nTotal images loaded: {len(image_paths)}\")\n\n# Split the dataset\ntrain_paths, test_paths, train_labels, test_labels = train_test_split(\n    image_paths, labels, test_size=0.2, stratify=labels, random_state=42\n)\n\n# Show split counts\nprint(f\"Training images: {len(train_paths)}\")\nprint(f\"Testing images: {len(test_paths)}\")\n","metadata":{"id":"1L-VpxLo6Tkc","outputId":"00707175-8e39-4c99-afd2-38adba7cd584","trusted":true,"execution":{"iopub.status.busy":"2025-04-26T00:29:41.395460Z","iopub.execute_input":"2025-04-26T00:29:41.395645Z","iopub.status.idle":"2025-04-26T00:29:43.969763Z","shell.execute_reply.started":"2025-04-26T00:29:41.395629Z","shell.execute_reply":"2025-04-26T00:29:43.968888Z"}},"outputs":[{"name":"stdout","text":"Detected classes: ['Mild Dementia', 'Moderate Dementia', 'Non Demented', 'Very mild Dementia']\nClass: Mild Dementia, Files Found: 5002\nClass: Moderate Dementia, Files Found: 488\nClass: Non Demented, Files Found: 67222\nClass: Very mild Dementia, Files Found: 13725\n\nTotal images loaded: 86437\nTraining images: 69149\nTesting images: 17288\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import tensorflow as tf\nfrom sklearn.preprocessing import LabelBinarizer\n\n# One-hot encode labels\ntrain_paths = np.array(train_paths)\ntest_paths = np.array(test_paths)\n\nlb = LabelBinarizer()\ntrain_labels = lb.fit_transform(train_labels)\ntest_labels = lb.transform(test_labels)\n\nIMG_HEIGHT, IMG_WIDTH = 224, 224\n\ndef preprocess_image(image_path):\n    image = tf.io.read_file(image_path)\n    image = tf.image.decode_jpeg(image, channels=3)\n    image = tf.image.resize(image, [IMG_HEIGHT, IMG_WIDTH])\n    return image / 255.0\n\ndef load_dataset(image_paths, labels):\n    dataset = tf.data.Dataset.from_tensor_slices((image_paths, labels))\n    dataset = dataset.map(lambda x, y: (preprocess_image(x), y))\n    return dataset\n\n# Load datasets\ntrain_dataset = load_dataset(train_paths, train_labels).shuffle(1000).batch(32).prefetch(tf.data.AUTOTUNE)\ntest_dataset = load_dataset(test_paths, test_labels).batch(32).prefetch(tf.data.AUTOTUNE)\n\n# Print dataset objects\nprint(\"Train Dataset: 224x224 images with 4 class labels : Mild, Moderate, Non, Very Mild Dementia\")\nprint(\"Test Dataset: 224x224 images with 4 class labels : Mild, Moderate, Non, Very Mild Dementia\")\n\n","metadata":{"id":"Zkwx9PktCHum","outputId":"63760cb8-d058-442b-f956-6c9c5a446b7b","trusted":true,"execution":{"iopub.status.busy":"2025-04-26T00:29:43.971796Z","iopub.execute_input":"2025-04-26T00:29:43.972141Z","iopub.status.idle":"2025-04-26T00:30:01.189295Z","shell.execute_reply.started":"2025-04-26T00:29:43.972121Z","shell.execute_reply":"2025-04-26T00:30:01.188485Z"}},"outputs":[{"name":"stderr","text":"2025-04-26 00:29:46.186657: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1745627386.517848      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1745627386.600835      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Train Dataset: 224x224 images with 4 class labels : Mild, Moderate, Non, Very Mild Dementia\nTest Dataset: 224x224 images with 4 class labels : Mild, Moderate, Non, Very Mild Dementia\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1745627401.024659      31 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\nI0000 00:00:1745627401.025386      31 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13942 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from tensorflow.keras import layers, models\n\nmodel = models.Sequential([\n    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),\n    layers.MaxPooling2D(2, 2),\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.MaxPooling2D(2, 2),\n    layers.Conv2D(128, (3, 3), activation='relu'),\n    layers.MaxPooling2D(2, 2),\n    layers.Flatten(),\n    layers.Dense(128, activation='relu'),\n    layers.Dropout(0.5),\n    layers.Dense(len(classes), activation='softmax')\n])\n\n# Print the model summary\nprint(\"CNN Model Architecture:\")\nmodel.summary()\n","metadata":{"id":"hTWmgxn2CQre","outputId":"35999c62-c464-4461-97e4-26f542b65004","trusted":true,"execution":{"iopub.status.busy":"2025-04-26T00:30:01.190134Z","iopub.execute_input":"2025-04-26T00:30:01.190623Z","iopub.status.idle":"2025-04-26T00:30:02.699336Z","shell.execute_reply.started":"2025-04-26T00:30:01.190595Z","shell.execute_reply":"2025-04-26T00:30:02.698583Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"CNN Model Architecture:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\nâ”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\nâ”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\nâ”‚ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                      â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m222\u001b[0m, \u001b[38;5;34m222\u001b[0m, \u001b[38;5;34m32\u001b[0m)        â”‚             \u001b[38;5;34m896\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)         â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m111\u001b[0m, \u001b[38;5;34m111\u001b[0m, \u001b[38;5;34m32\u001b[0m)        â”‚               \u001b[38;5;34m0\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)                    â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m109\u001b[0m, \u001b[38;5;34m109\u001b[0m, \u001b[38;5;34m64\u001b[0m)        â”‚          \u001b[38;5;34m18,496\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m54\u001b[0m, \u001b[38;5;34m54\u001b[0m, \u001b[38;5;34m64\u001b[0m)          â”‚               \u001b[38;5;34m0\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)                    â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m52\u001b[0m, \u001b[38;5;34m52\u001b[0m, \u001b[38;5;34m128\u001b[0m)         â”‚          \u001b[38;5;34m73,856\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m128\u001b[0m)         â”‚               \u001b[38;5;34m0\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ flatten (\u001b[38;5;33mFlatten\u001b[0m)                    â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m86528\u001b[0m)               â”‚               \u001b[38;5;34m0\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense (\u001b[38;5;33mDense\u001b[0m)                        â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 â”‚      \u001b[38;5;34m11,075,712\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout (\u001b[38;5;33mDropout\u001b[0m)                    â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 â”‚               \u001b[38;5;34m0\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)                   â”‚             \u001b[38;5;34m516\u001b[0m â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\nâ”ƒ<span style=\"font-weight: bold\"> Layer (type)                         </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape                </span>â”ƒ<span style=\"font-weight: bold\">         Param # </span>â”ƒ\nâ”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\nâ”‚ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                      â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">222</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">222</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)         â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">111</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">111</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">109</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">109</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">54</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">54</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                    â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">86528</span>)               â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 â”‚      <span style=\"color: #00af00; text-decoration-color: #00af00\">11,075,712</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)                   â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">516</span> â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m11,169,476\u001b[0m (42.61 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">11,169,476</span> (42.61 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m11,169,476\u001b[0m (42.61 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">11,169,476</span> (42.61 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n\n# Set up mirrored strategy for multi-GPU\nstrategy = tf.distribute.MirroredStrategy()\nprint(f\"Number of devices: {strategy.num_replicas_in_sync}\")\n\n# Create and compile the model inside the strategy scope\nwith strategy.scope():\n    \n    model = Sequential([\n        Conv2D(32, (3,3), activation='relu', input_shape=(224, 224, 3)),\n        MaxPooling2D(),\n        Flatten(),\n        Dense(128, activation='relu'),\n        Dense(4, activation='softmax')  # 4 classes\n    ])\n\n    model.compile(optimizer=Adam(learning_rate=0.001),\n                  loss='categorical_crossentropy',\n                  metrics=['accuracy'])\n\n# Train the model\nEPOCHS = 3\nprint(\"Starting training...\")\nhistory = model.fit(train_dataset, validation_data=test_dataset, epochs=EPOCHS)\n","metadata":{"id":"ruAdNvarCV_T","outputId":"e5e5aa9b-848f-4777-a039-d0b7a96420a0","trusted":true,"execution":{"iopub.status.busy":"2025-04-26T00:30:02.700234Z","iopub.execute_input":"2025-04-26T00:30:02.700818Z"}},"outputs":[{"name":"stdout","text":"Number of devices: 2\nStarting training...\nEpoch 1/3\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1745627409.781323      94 cuda_dnn.cc:529] Loaded cuDNN version 90300\nI0000 00:00:1745627409.787110      93 cuda_dnn.cc:529] Loaded cuDNN version 90300\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m2161/2161\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m277s\u001b[0m 126ms/step - accuracy: 0.8899 - loss: 0.4667 - val_accuracy: 0.9904 - val_loss: 0.0258\nEpoch 2/3\n\u001b[1m2161/2161\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m241s\u001b[0m 111ms/step - accuracy: 0.9932 - loss: 0.0198 - val_accuracy: 0.9943 - val_loss: 0.0174\nEpoch 3/3\n\u001b[1m 626/2161\u001b[0m \u001b[32mâ”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m2:42\u001b[0m 106ms/step - accuracy: 0.9954 - loss: 0.0114","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"# Final evaluation\nloss, accuracy = model.evaluate(test_dataset)\nprint(f\"Final Test Accuracy: {accuracy:.4f}\")\nprint(f\"Final Test Loss: {loss:.4f}\")\n\n# Save model for future use\nmodel.save(\"/kaggle/working/alzheimers_model.h5\")\nprint(\"Model saved as 'alzheimers_model.h5' in /kaggle/working/\")\n","metadata":{"id":"YuXYl-BEqQDh","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 4))\nplt.plot(history.history['accuracy'], label='Train Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.title('Model Accuracy Over Epochs')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.grid(True)\nplt.show()","metadata":{"id":"g3LevDxhqU6N","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tensorflow.keras.models import load_model\n\n# Load your trained model\nmodel = load_model(\"/kaggle/working/alzheimers_model.h5\")  # or your CNN model path\n\n# Define classes\nclasses = [\"Mild Dementia\", \"Moderate Dementia\", \"Non Demented\", \"Very mild Dementia\"]\n\n# Preprocess single image (same as training)\ndef preprocess_image(image_path):\n    image = tf.io.read_file(image_path)\n    image = tf.image.decode_jpeg(image, channels=3)\n    image = tf.image.resize(image, [224, 224])\n    return image / 255.0\n\n# ğŸ” Replace this with your own image path!\ntest_image_path = \"/kaggle/input/imagesoasis/Data/Moderate Dementia/OAS1_0308_MR1_mpr-1_105.jpg\"\n\n# Preprocess and predict\nimage = preprocess_image(test_image_path)\nimage = tf.expand_dims(image, axis=0)  # Add batch dimension\nprediction = model.predict(image)\n\n# Get predicted class\npredicted_index = np.argmax(prediction)\npredicted_label = classes[predicted_index]\n\n# Output result\nprint(f\"Predicted Class: {predicted_label}\")\nprint(f\"Prediction Probabilities: {prediction}\")\n\n# Show image\nplt.imshow(plt.imread(test_image_path))\nplt.title(f\"Predicted: {predicted_label}\")\nplt.axis('off')\nplt.show()\n","metadata":{"id":"5_E8K-EmrIXP","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Extract true labels from test_dataset\ntrue_labels = []\nfor _, label_batch in test_dataset:\n    true_labels.extend(np.argmax(label_batch.numpy(), axis=1))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Predict with CNN\ncnn_preds = model.predict(test_dataset)\ncnn_pred_labels = np.argmax(cnn_preds, axis=1)\n\n# Extract true labels\ntrue_labels = []\nfor _, labels in test_dataset:\n    true_labels.extend(np.argmax(labels.numpy(), axis=1))\ntrue_labels = np.array(true_labels)\n\n# Evaluate CNN\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n\ncnn_acc = accuracy_score(true_labels, cnn_pred_labels)\ncnn_cm = confusion_matrix(true_labels, cnn_pred_labels)\ncnn_cr = classification_report(true_labels, cnn_pred_labels, target_names=classes)\n\nprint(f\"CNN Accuracy: {cnn_acc:.2f}\")\nprint(\"\\nCNN Confusion Matrix:\\n\", cnn_cm)\nprint(\"\\nCNN Classification Report:\\n\", cnn_cr)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save CNN classification report and confusion matrix to CSV\npd.DataFrame(classification_report(true_labels, cnn_pred_labels, output_dict=True)).transpose().to_csv(\"/content/cnn_report.csv\")\npd.DataFrame(cnn_cm).to_csv(\"/content/cnn_confusion_matrix.csv\", index=False)\n\nprint(\"CNN metrics saved to 'cnn_report.csv' and 'cnn_confusion_matrix.csv'\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Final evaluation on test data\nloss, accuracy = model.evaluate(test_dataset)\nprint(f\"Final Test Accuracy: {accuracy:.4f}\")\nprint(f\"Final Test Loss: {loss:.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Plot Accuracy and Loss side-by-side\nplt.figure(figsize=(12, 4))\n\n# Accuracy\nplt.subplot(1, 2, 1)\nplt.plot(history.history['accuracy'], label='Train Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.title('Model Accuracy During Training')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.grid(True)\n\n# Loss\nplt.subplot(1, 2, 2)\nplt.plot(history.history['loss'], label='Train Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.title('Model Loss During Training')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.grid(True)\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(model.input_shape)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport random\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\n\n# Set the path to the dataset\ndata_dir = \"/kaggle/input/imagesoasis/Data\"  # change if needed\ncategories = os.listdir(data_dir)\n\n# Plot setup\nplt.figure(figsize=(15, 4))\n\nfor i, category in enumerate(categories):\n    category_path = os.path.join(data_dir, category)\n    images = [img for img in os.listdir(category_path) if img.endswith('.jpg')]\n    chosen_img = random.choice(images)\n    img_path = os.path.join(category_path, chosen_img)\n\n    # Load and display image\n    img = tf.keras.utils.load_img(img_path, target_size=(128, 128))\n    plt.subplot(1, len(categories), i + 1)\n    plt.imshow(img)\n    plt.axis('off')\n    plt.title(category)\n\nplt.suptitle(\"Sample MRI Images from Each Class\")\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"VGG16","metadata":{}},{"cell_type":"code","source":"import os\nimport glob\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelBinarizer\nimport tensorflow as tf\n\n# Folder path and classes\ndataset_dir = \"/kaggle/input/imagesoasis/Data\"\nclasses = [\"Non Demented\", \"Very mild Dementia\", \"Mild Dementia\", \"Moderate Dementia\"]\n\n# Prepare image paths and binary labels (0 = Non Demented, 1 = Demented)\nimage_paths = []\nlabels = []\n\nfor class_name in classes:\n    label = 0 if class_name == \"Non Demented\" else 1\n    class_path = os.path.join(dataset_dir, class_name)\n    files = glob.glob(f\"{class_path}/*.jpg\")\n    \n    image_paths.extend(files)\n    labels.extend([label] * len(files))\n\n# Report counts\nprint(\"Total Images Found:\", len(image_paths))\nprint(\"Class Distribution:\")\nprint(f\"  Non Demented (0): {labels.count(0)}\")\nprint(f\"  Demented (1): {labels.count(1)}\")\n\n# Train-test split\ntrain_paths, test_paths, train_labels, test_labels = train_test_split(\n    image_paths, labels, test_size=0.2, stratify=labels, random_state=42\n)\n\nprint(\"\\n Split:\")\nprint(\"  Train Samples:\", len(train_paths))\nprint(\"  Test Samples:\", len(test_paths))\n\n# Preprocessing functions\nIMG_HEIGHT, IMG_WIDTH = 128, 128\n\ndef preprocess_image(image_path):\n    image = tf.io.read_file(image_path)\n    image = tf.image.decode_jpeg(image, channels=3)\n    image = tf.image.resize(image, [IMG_HEIGHT, IMG_WIDTH])\n    return image / 255.0\n\ndef load_dataset(image_paths, labels):\n    dataset = tf.data.Dataset.from_tensor_slices((image_paths, labels))\n    dataset = dataset.map(lambda x, y: (preprocess_image(x), y))\n    return dataset\n\n# Load datasets\nBATCH_SIZE = 32\ntrain_dataset = load_dataset(train_paths, train_labels).shuffle(1000).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\ntest_dataset = load_dataset(test_paths, test_labels).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.applications import VGG16\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Flatten, Dense, Dropout\nfrom tensorflow.keras.optimizers import Adam\n\nvgg_base = VGG16(weights='imagenet', include_top=False, input_shape=(128, 128, 3))\nvgg_base.trainable = False  # Freeze VGG16 layers\n\nmodel = Sequential([\n    vgg_base,\n    Flatten(),\n    Dense(128, activation='relu'),\n    Dropout(0.5),\n    Dense(1, activation='sigmoid')  # Binary output\n])\n\nmodel.compile(\n    optimizer=Adam(learning_rate=0.0001),\n    loss='binary_crossentropy',\n    metrics=['accuracy']\n)\n\nmodel.summary()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"history = model.fit(\n    train_dataset,\n    validation_data=test_dataset,\n    epochs=10\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"loss, accuracy = model.evaluate(test_dataset)\nprint(f\" Binary Detection Accuracy: {accuracy:.4f}\")\n\n# Plot training history\nimport matplotlib.pyplot as plt\n\nplt.plot(history.history['accuracy'], label=\"Train Acc\")\nplt.plot(history.history['val_accuracy'], label=\"Val Acc\")\nplt.legend()\nplt.title(\"VGG16 Binary Accuracy\")\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Plot Accuracy\nplt.figure(figsize=(12, 4))\nplt.subplot(1, 2, 1)\nplt.plot(history.history['accuracy'], label='Train Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.title('VGG16 Binary Classification - Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.grid(True)\n\n# Plot Loss\nplt.subplot(1, 2, 2)\nplt.plot(history.history['loss'], label='Train Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.title('VGG16 Binary Classification - Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.grid(True)\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.models import load_model\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Load the saved model\nmodel = load_model(\"/kaggle/working/vgg16_dementia_binary.h5\")\n\n# Define class labels (for binary setup)\nlabels = [\"Non Demented\", \"Demented\"]  # 0: Non, 1: Demented\n\n\ndef preprocess_image(image_path):\n    img = tf.io.read_file(image_path)\n    img = tf.image.decode_jpeg(img, channels=3)\n    img = tf.image.resize(img, [128, 128])  # â† changed from 224 to 128\n    img = img / 255.0\n    return tf.expand_dims(img, axis=0)\n\n\n# ğŸ” Replace this path with any image you want to test\ntest_image_path = \"//kaggle/input/imagesoasis/Data/Moderate Dementia/OAS1_0308_MR1_mpr-1_100.jpg\"\n\n# Preprocess and predict\nimg = preprocess_image(test_image_path)\nprediction = model.predict(img)[0][0]\n\n# Determine class\npredicted_class = 1 if prediction >= 0.5 else 0\nconfidence = prediction if predicted_class == 1 else 1 - prediction\nlabel = labels[predicted_class]\n\n# Output result\nprint(f\"Predicted Class: {label}\")\nprint(f\"Confidence: {confidence:.2f}\")\n\n# Show the image\nplt.imshow(plt.imread(test_image_path))\nplt.title(f\"Predicted: {label} ({confidence:.2f})\")\nplt.axis('off')\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}